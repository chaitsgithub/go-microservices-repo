# ==============================
# Global Settings (Optional)
# ==============================
# Path settings can also go in pipelines.yml
# path.data: /var/lib/logstash
# path.logs: /var/log/logstash

# ==============================
# Pipeline Definition
# ==============================
input {
  # --- File input ---
  # file {
  #   path => ["/var/log/*.log"]    # Array of paths
  #   start_position => "beginning" # beginning / end
  #   sincedb_path => "/dev/null"   # Disable sincedb tracking
  #   codec => "plain"              # Codec for decoding (plain/json/multiline)
  #   mode => "tail"                # read / tail
  #   discover_interval => 15
  #   stat_interval => 1
  # }

  # --- Stdin input ---
  # stdin {
  #   codec => "json"                # or "plain"
  # }

  # --- Beats input (Filebeat/Winlogbeat) ---
  beats {
    port => 5044
    ssl => false
    # ssl_certificate => "/etc/logstash/cert.pem"
    # ssl_key => "/etc/logstash/key.pem"
  }

  # --- Kafka input ---
  # kafka {
  #   bootstrap_servers => "localhost:9092"
  #   topics => ["logs"]
  #   group_id => "logstash"
  #   auto_offset_reset => "earliest"
  #   decorate_events => true
  #   codec => "json"
  # }

  # --- HTTP input ---
  # http {
  #   port => 8080
  #   ssl => false
  # }

  # --- TCP/UDP inputs ---
  tcp {
    port => 5000
    codec => "json_lines"
  }
  # udp {
  #   port => 5001
  #   codec => "plain"
  # }
}

# filter {
#   # --- Grok ---
#   grok {
#     match => { "message" => "%{COMBINEDAPACHELOG}" }
#     overwrite => [ "message" ]
#     tag_on_failure => ["_grokparsefailure"]
#   }

#   # --- Date parsing ---
#   date {
#     match => ["timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss,SSS"]
#     timezone => "UTC"
#     target => "@timestamp"
#   }

#   # --- Mutate ---
#   mutate {
#     rename => { "host" => "hostname" }
#     remove_field => ["path"]
#     add_field => { "environment" => "production" }
#     convert => { "status" => "integer" }
#     gsub => ["message", "\\t", " "]  # replace tabs with spaces
#   }

#   # --- GeoIP ---
#   geoip {
#     source => "client_ip"
#     target => "geoip"
#     database => "/etc/logstash/GeoLite2-City.mmdb"
#   }

#   # --- JSON decode ---
#   json {
#     source => "message"
#     target => "json_data"
#   }

#   # --- Drop unwanted events ---
#   if [status] == 200 {
#     drop { }
#   }

#   # --- Translate (lookup) ---
#   translate {
#     field => "status"
#     destination => "status_description"
#     dictionary => {
#       "200" => "OK"
#       "404" => "Not Found"
#     }
#   }

#   # --- Conditionals ---
#   if "ERROR" in [message] {
#     mutate { add_tag => ["error_log"] }
#   }
# }

output {
  # --- Stdout ---
  # stdout {
  #   codec => rubydebug { metadata => true }
  # }

  # --- Elasticsearch ---
  # elasticsearch {
  #   hosts => ["http://localhost:9200"]
  #   index => "logs-%{+YYYY.MM.dd}"
  #   user => "elastic"
  #   password => "changeme"
  #   ssl => false
  #   ilm_enabled => false
  # }
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }

  # --- File output ---
  # file {
  #   path => "/var/log/logstash/output.log"
  #   codec => line { format => "custom format: %{message}" }
  # }

  # --- Kafka output ---
  # kafka {
  #   bootstrap_servers => "localhost:9092"
  #   topic_id => "processed-logs"
  #   codec => "json"
  #   acks => "all"
  # }

  # --- HTTP output ---
  # http {
  #   url => "http://example.com/ingest"
  #   http_method => "post"
  #   format => "json"
  #   mapping => {
  #     "host" => "%{host}"
  #     "message" => "%{message}"
  #   }
  # }

  # --- Null output (discard) ---
  # null { }
}
